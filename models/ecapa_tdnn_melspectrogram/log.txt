G:\envs\py38\python.exe C:/jupyterNoteBook/VoiceprintRecognition-Pytorch/train.py 
-----------  Configuration Arguments -----------
augment_conf_path: configs/augment.yml
batch_size: 64
feature_method: melspectrogram
gpus: 0
learning_rate: 0.001
num_epoch: 30
num_speakers: 3242
num_workers: 8
pretrained_model: None
resume: models/ecapa_tdnn_melspectrogram
save_model_dir: models/
test_list_path: dataset/test_list.txt
train_list_path: dataset/train_list.txt
use_model: ecapa_tdnn
------------------------------------------------
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv1d-1              [-1, 512, 98]         204,800
       BatchNorm1d-2              [-1, 512, 98]           1,024
      Conv1dReluBn-3              [-1, 512, 98]               0
            Conv1d-4              [-1, 512, 98]         262,144
       BatchNorm1d-5              [-1, 512, 98]           1,024
      Conv1dReluBn-6              [-1, 512, 98]               0
            Conv1d-7               [-1, 64, 98]          12,288
       BatchNorm1d-8               [-1, 64, 98]             128
            Conv1d-9               [-1, 64, 98]          12,288
      BatchNorm1d-10               [-1, 64, 98]             128
           Conv1d-11               [-1, 64, 98]          12,288
      BatchNorm1d-12               [-1, 64, 98]             128
           Conv1d-13               [-1, 64, 98]          12,288
      BatchNorm1d-14               [-1, 64, 98]             128
           Conv1d-15               [-1, 64, 98]          12,288
      BatchNorm1d-16               [-1, 64, 98]             128
           Conv1d-17               [-1, 64, 98]          12,288
      BatchNorm1d-18               [-1, 64, 98]             128
           Conv1d-19               [-1, 64, 98]          12,288
      BatchNorm1d-20               [-1, 64, 98]             128
 Res2Conv1dReluBn-21              [-1, 512, 98]               0
           Conv1d-22              [-1, 512, 98]         262,144
      BatchNorm1d-23              [-1, 512, 98]           1,024
     Conv1dReluBn-24              [-1, 512, 98]               0
           Linear-25                  [-1, 256]         131,328
           Linear-26                  [-1, 512]         131,584
       SE_Connect-27              [-1, 512, 98]               0
           Conv1d-28              [-1, 512, 98]         262,144
      BatchNorm1d-29              [-1, 512, 98]           1,024
     Conv1dReluBn-30              [-1, 512, 98]               0
           Conv1d-31               [-1, 64, 98]          12,288
      BatchNorm1d-32               [-1, 64, 98]             128
           Conv1d-33               [-1, 64, 98]          12,288
      BatchNorm1d-34               [-1, 64, 98]             128
           Conv1d-35               [-1, 64, 98]          12,288
      BatchNorm1d-36               [-1, 64, 98]             128
           Conv1d-37               [-1, 64, 98]          12,288
      BatchNorm1d-38               [-1, 64, 98]             128
           Conv1d-39               [-1, 64, 98]          12,288
      BatchNorm1d-40               [-1, 64, 98]             128
           Conv1d-41               [-1, 64, 98]          12,288
      BatchNorm1d-42               [-1, 64, 98]             128
           Conv1d-43               [-1, 64, 98]          12,288
      BatchNorm1d-44               [-1, 64, 98]             128
 Res2Conv1dReluBn-45              [-1, 512, 98]               0
           Conv1d-46              [-1, 512, 98]         262,144
      BatchNorm1d-47              [-1, 512, 98]           1,024
     Conv1dReluBn-48              [-1, 512, 98]               0
           Linear-49                  [-1, 256]         131,328
           Linear-50                  [-1, 512]         131,584
       SE_Connect-51              [-1, 512, 98]               0
           Conv1d-52              [-1, 512, 98]         262,144
      BatchNorm1d-53              [-1, 512, 98]           1,024
     Conv1dReluBn-54              [-1, 512, 98]               0
           Conv1d-55               [-1, 64, 98]          12,288
      BatchNorm1d-56               [-1, 64, 98]             128
           Conv1d-57               [-1, 64, 98]          12,288
      BatchNorm1d-58               [-1, 64, 98]             128
           Conv1d-59               [-1, 64, 98]          12,288
      BatchNorm1d-60               [-1, 64, 98]             128
           Conv1d-61               [-1, 64, 98]          12,288
      BatchNorm1d-62               [-1, 64, 98]             128
           Conv1d-63               [-1, 64, 98]          12,288
      BatchNorm1d-64               [-1, 64, 98]             128
           Conv1d-65               [-1, 64, 98]          12,288
      BatchNorm1d-66               [-1, 64, 98]             128
           Conv1d-67               [-1, 64, 98]          12,288
      BatchNorm1d-68               [-1, 64, 98]             128
 Res2Conv1dReluBn-69              [-1, 512, 98]               0
           Conv1d-70              [-1, 512, 98]         262,144
      BatchNorm1d-71              [-1, 512, 98]           1,024
     Conv1dReluBn-72              [-1, 512, 98]               0
           Linear-73                  [-1, 256]         131,328
           Linear-74                  [-1, 512]         131,584
       SE_Connect-75              [-1, 512, 98]               0
           Conv1d-76             [-1, 1536, 98]       2,360,832
           Conv1d-77              [-1, 128, 98]         196,736
           Conv1d-78             [-1, 1536, 98]         198,144
AttentiveStatsPool-79                 [-1, 3072]               0
      BatchNorm1d-80                 [-1, 3072]           6,144
           Linear-81                  [-1, 192]         590,016
      BatchNorm1d-82                  [-1, 192]             384
        EcapaTdnn-83                  [-1, 192]               0
          Dropout-84                  [-1, 192]               0
================================================================
Total params: 6,186,560
Trainable params: 6,186,560
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.03
Forward/backward pass size (MB): 14.81
Params size (MB): 23.60
Estimated Total Size (MB): 38.44
----------------------------------------------------------------
成功加载第 29 轮的模型参数和优化方法参数
[2022-09-27 23:17:30.474799] Train epoch [29/30], batch: [0/16579], loss: 0.22958, accuracy: 1.00000, lr: 0.00017584, eta: 13 days, 4:58:21
[2022-09-27 23:17:38.472803] Train epoch [29/30], batch: [100/16579], loss: 0.40600, accuracy: 0.99443, lr: 0.00017584, eta: 0:21:41
[2022-09-27 23:17:46.534803] Train epoch [29/30], batch: [200/16579], loss: 0.39472, accuracy: 0.99417, lr: 0.00017584, eta: 0:22:06
[2022-09-27 23:17:54.595801] Train epoch [29/30], batch: [300/16579], loss: 0.39948, accuracy: 0.99403, lr: 0.00017584, eta: 0:23:03
[2022-09-27 23:18:02.609802] Train epoch [29/30], batch: [400/16579], loss: 0.39913, accuracy: 0.99361, lr: 0.00017584, eta: 0:21:18
[2022-09-27 23:18:10.678803] Train epoch [29/30], batch: [500/16579], loss: 0.39797, accuracy: 0.99326, lr: 0.00017584, eta: 0:20:54
[2022-09-27 23:18:18.667800] Train epoch [29/30], batch: [600/16579], loss: 0.39672, accuracy: 0.99342, lr: 0.00017584, eta: 0:21:02
[2022-09-27 23:18:26.754804] Train epoch [29/30], batch: [700/16579], loss: 0.39689, accuracy: 0.99336, lr: 0.00017584, eta: 0:21:10
[2022-09-27 23:18:34.804800] Train epoch [29/30], batch: [800/16579], loss: 0.39918, accuracy: 0.99313, lr: 0.00017584, eta: 0:22:52
[2022-09-27 23:18:42.808802] Train epoch [29/30], batch: [900/16579], loss: 0.40192, accuracy: 0.99303, lr: 0.00017584, eta: 0:20:22
[2022-09-27 23:18:50.876804] Train epoch [29/30], batch: [1000/16579], loss: 0.40447, accuracy: 0.99298, lr: 0.00017584, eta: 0:18:57
[2022-09-27 23:18:58.877802] Train epoch [29/30], batch: [1100/16579], loss: 0.40406, accuracy: 0.99296, lr: 0.00017584, eta: 0:21:24
[2022-09-27 23:19:06.908800] Train epoch [29/30], batch: [1200/16579], loss: 0.40363, accuracy: 0.99296, lr: 0.00017584, eta: 0:20:14
[2022-09-27 23:19:14.953800] Train epoch [29/30], batch: [1300/16579], loss: 0.40518, accuracy: 0.99294, lr: 0.00017584, eta: 0:21:23
[2022-09-27 23:19:22.918315] Train epoch [29/30], batch: [1400/16579], loss: 0.40657, accuracy: 0.99284, lr: 0.00017584, eta: 0:19:59
[2022-09-27 23:19:30.988317] Train epoch [29/30], batch: [1500/16579], loss: 0.40741, accuracy: 0.99277, lr: 0.00017584, eta: 0:19:36
[2022-09-27 23:19:38.993315] Train epoch [29/30], batch: [1600/16579], loss: 0.40741, accuracy: 0.99278, lr: 0.00017584, eta: 0:20:43
[2022-09-27 23:19:47.032316] Train epoch [29/30], batch: [1700/16579], loss: 0.40822, accuracy: 0.99272, lr: 0.00017584, eta: 0:19:35
[2022-09-27 23:19:55.097314] Train epoch [29/30], batch: [1800/16579], loss: 0.40946, accuracy: 0.99261, lr: 0.00017584, eta: 0:19:57
[2022-09-27 23:20:03.075317] Train epoch [29/30], batch: [1900/16579], loss: 0.40901, accuracy: 0.99264, lr: 0.00017584, eta: 0:19:19
[2022-09-27 23:20:11.127314] Train epoch [29/30], batch: [2000/16579], loss: 0.40912, accuracy: 0.99265, lr: 0.00017584, eta: 0:17:58
[2022-09-27 23:20:19.136315] Train epoch [29/30], batch: [2100/16579], loss: 0.40949, accuracy: 0.99267, lr: 0.00017584, eta: 0:20:16
[2022-09-27 23:20:27.165315] Train epoch [29/30], batch: [2200/16579], loss: 0.40917, accuracy: 0.99272, lr: 0.00017584, eta: 0:19:10
[2022-09-27 23:20:35.236320] Train epoch [29/30], batch: [2300/16579], loss: 0.40895, accuracy: 0.99269, lr: 0.00017584, eta: 0:19:45
[2022-09-27 23:20:43.242318] Train epoch [29/30], batch: [2400/16579], loss: 0.40914, accuracy: 0.99276, lr: 0.00017584, eta: 0:19:08
[2022-09-27 23:20:51.291315] Train epoch [29/30], batch: [2500/16579], loss: 0.40914, accuracy: 0.99275, lr: 0.00017584, eta: 0:18:32
[2022-09-27 23:20:59.307315] Train epoch [29/30], batch: [2600/16579], loss: 0.40941, accuracy: 0.99276, lr: 0.00017584, eta: 0:18:38
[2022-09-27 23:21:07.343314] Train epoch [29/30], batch: [2700/16579], loss: 0.41028, accuracy: 0.99272, lr: 0.00017584, eta: 0:18:02
[2022-09-27 23:21:15.398317] Train epoch [29/30], batch: [2800/16579], loss: 0.41057, accuracy: 0.99274, lr: 0.00017584, eta: 0:17:54
[2022-09-27 23:21:23.380317] Train epoch [29/30], batch: [2900/16579], loss: 0.41057, accuracy: 0.99267, lr: 0.00017584, eta: 0:18:00
[2022-09-27 23:21:31.459316] Train epoch [29/30], batch: [3000/16579], loss: 0.41068, accuracy: 0.99268, lr: 0.00017584, eta: 0:17:52
[2022-09-27 23:21:39.514317] Train epoch [29/30], batch: [3100/16579], loss: 0.41056, accuracy: 0.99271, lr: 0.00017584, eta: 0:19:05
[2022-09-27 23:21:47.542315] Train epoch [29/30], batch: [3200/16579], loss: 0.41080, accuracy: 0.99268, lr: 0.00017584, eta: 0:17:50
[2022-09-27 23:21:55.640315] Train epoch [29/30], batch: [3300/16579], loss: 0.41068, accuracy: 0.99272, lr: 0.00017584, eta: 0:17:42
[2022-09-27 23:22:03.656314] Train epoch [29/30], batch: [3400/16579], loss: 0.41062, accuracy: 0.99271, lr: 0.00017584, eta: 0:17:34
[2022-09-27 23:22:11.759316] Train epoch [29/30], batch: [3500/16579], loss: 0.41105, accuracy: 0.99270, lr: 0.00017584, eta: 0:17:26
[2022-09-27 23:22:19.850314] Train epoch [29/30], batch: [3600/16579], loss: 0.41084, accuracy: 0.99267, lr: 0.00017584, eta: 0:18:23
[2022-09-27 23:22:27.893317] Train epoch [29/30], batch: [3700/16579], loss: 0.41068, accuracy: 0.99271, lr: 0.00017584, eta: 0:16:57
[2022-09-27 23:22:35.978313] Train epoch [29/30], batch: [3800/16579], loss: 0.41065, accuracy: 0.99267, lr: 0.00017584, eta: 0:16:49
[2022-09-27 23:22:43.994316] Train epoch [29/30], batch: [3900/16579], loss: 0.41091, accuracy: 0.99263, lr: 0.00017584, eta: 0:17:57
[2022-09-27 23:22:52.044315] Train epoch [29/30], batch: [4000/16579], loss: 0.41087, accuracy: 0.99267, lr: 0.00017584, eta: 0:16:08
[2022-09-27 23:23:00.113317] Train epoch [29/30], batch: [4100/16579], loss: 0.41176, accuracy: 0.99262, lr: 0.00017584, eta: 0:17:03
[2022-09-27 23:23:08.122316] Train epoch [29/30], batch: [4200/16579], loss: 0.41162, accuracy: 0.99260, lr: 0.00017584, eta: 0:16:17
[2022-09-27 23:23:16.312317] Train epoch [29/30], batch: [4300/16579], loss: 0.41156, accuracy: 0.99258, lr: 0.00017584, eta: 0:16:10
[2022-09-27 23:23:24.323316] Train epoch [29/30], batch: [4400/16579], loss: 0.41213, accuracy: 0.99253, lr: 0.00017584, eta: 0:16:02
[2022-09-27 23:23:32.330314] Train epoch [29/30], batch: [4500/16579], loss: 0.41215, accuracy: 0.99256, lr: 0.00017584, eta: 0:15:54
[2022-09-27 23:23:40.516316] Train epoch [29/30], batch: [4600/16579], loss: 0.41237, accuracy: 0.99253, lr: 0.00017584, eta: 0:15:46
[2022-09-27 23:23:48.475316] Train epoch [29/30], batch: [4700/16579], loss: 0.41291, accuracy: 0.99250, lr: 0.00017584, eta: 0:15:38
[2022-09-27 23:23:56.498316] Train epoch [29/30], batch: [4800/16579], loss: 0.41290, accuracy: 0.99251, lr: 0.00017584, eta: 0:15:18
[2022-09-27 23:24:04.518315] Train epoch [29/30], batch: [4900/16579], loss: 0.41315, accuracy: 0.99253, lr: 0.00017584, eta: 0:15:34
[2022-09-27 23:24:12.544315] Train epoch [29/30], batch: [5000/16579], loss: 0.41352, accuracy: 0.99251, lr: 0.00017584, eta: 0:15:14
[2022-09-27 23:24:20.597317] Train epoch [29/30], batch: [5100/16579], loss: 0.41392, accuracy: 0.99250, lr: 0.00017584, eta: 0:15:06
[2022-09-27 23:24:28.579319] Train epoch [29/30], batch: [5200/16579], loss: 0.41368, accuracy: 0.99252, lr: 0.00017584, eta: 0:15:21
[2022-09-27 23:24:36.621316] Train epoch [29/30], batch: [5300/16579], loss: 0.41394, accuracy: 0.99252, lr: 0.00017584, eta: 0:14:51
[2022-09-27 23:24:44.680314] Train epoch [29/30], batch: [5400/16579], loss: 0.41407, accuracy: 0.99254, lr: 0.00017584, eta: 0:15:50
[2022-09-27 23:24:52.685316] Train epoch [29/30], batch: [5500/16579], loss: 0.41458, accuracy: 0.99249, lr: 0.00017584, eta: 0:14:24
[2022-09-27 23:25:00.722316] Train epoch [29/30], batch: [5600/16579], loss: 0.41456, accuracy: 0.99253, lr: 0.00017584, eta: 0:14:27
[2022-09-27 23:25:08.873316] Train epoch [29/30], batch: [5700/16579], loss: 0.41490, accuracy: 0.99252, lr: 0.00017584, eta: 0:15:24
[2022-09-27 23:25:17.488313] Train epoch [29/30], batch: [5800/16579], loss: 0.41506, accuracy: 0.99254, lr: 0.00017584, eta: 0:15:26
[2022-09-27 23:25:26.326317] Train epoch [29/30], batch: [5900/16579], loss: 0.41529, accuracy: 0.99254, lr: 0.00017584, eta: 0:15:29
[2022-09-27 23:25:35.179315] Train epoch [29/30], batch: [6000/16579], loss: 0.41554, accuracy: 0.99254, lr: 0.00017584, eta: 0:15:20
[2022-09-27 23:25:43.971314] Train epoch [29/30], batch: [6100/16579], loss: 0.41547, accuracy: 0.99254, lr: 0.00017584, eta: 0:16:04
[2022-09-27 23:25:52.815313] Train epoch [29/30], batch: [6200/16579], loss: 0.41598, accuracy: 0.99253, lr: 0.00017584, eta: 0:14:00
[2022-09-27 23:26:01.671314] Train epoch [29/30], batch: [6300/16579], loss: 0.41583, accuracy: 0.99256, lr: 0.00017584, eta: 0:14:43
[2022-09-27 23:26:10.527314] Train epoch [29/30], batch: [6400/16579], loss: 0.41593, accuracy: 0.99257, lr: 0.00017584, eta: 0:14:45
[2022-09-27 23:26:19.344316] Train epoch [29/30], batch: [6500/16579], loss: 0.41583, accuracy: 0.99261, lr: 0.00017584, eta: 0:14:46
[2022-09-27 23:26:28.180315] Train epoch [29/30], batch: [6600/16579], loss: 0.41565, accuracy: 0.99261, lr: 0.00017584, eta: 0:14:28
[2022-09-27 23:26:37.298316] Train epoch [29/30], batch: [6700/16579], loss: 0.41578, accuracy: 0.99260, lr: 0.00017584, eta: 0:14:29
[2022-09-27 23:26:46.883316] Train epoch [29/30], batch: [6800/16579], loss: 0.41569, accuracy: 0.99261, lr: 0.00017584, eta: 0:14:20
[2022-09-27 23:26:55.705315] Train epoch [29/30], batch: [6900/16579], loss: 0.41614, accuracy: 0.99260, lr: 0.00017584, eta: 0:14:11
[2022-09-27 23:27:04.379315] Train epoch [29/30], batch: [7000/16579], loss: 0.41626, accuracy: 0.99260, lr: 0.00017584, eta: 0:14:12
[2022-09-27 23:27:12.996317] Train epoch [29/30], batch: [7100/16579], loss: 0.41643, accuracy: 0.99260, lr: 0.00017584, eta: 0:13:25
[2022-09-27 23:27:21.649316] Train epoch [29/30], batch: [7200/16579], loss: 0.41640, accuracy: 0.99261, lr: 0.00017584, eta: 0:13:17
[2022-09-27 23:27:30.276316] Train epoch [29/30], batch: [7300/16579], loss: 0.41625, accuracy: 0.99263, lr: 0.00017584, eta: 0:13:18
[2022-09-27 23:27:38.878313] Train epoch [29/30], batch: [7400/16579], loss: 0.41623, accuracy: 0.99264, lr: 0.00017584, eta: 0:13:18
[2022-09-27 23:27:47.477316] Train epoch [29/30], batch: [7500/16579], loss: 0.41637, accuracy: 0.99263, lr: 0.00017584, eta: 0:12:06
[2022-09-27 23:27:56.069315] Train epoch [29/30], batch: [7600/16579], loss: 0.41669, accuracy: 0.99261, lr: 0.00017584, eta: 0:12:52
[2022-09-27 23:28:04.703315] Train epoch [29/30], batch: [7700/16579], loss: 0.41659, accuracy: 0.99261, lr: 0.00017584, eta: 0:13:01
[2022-09-27 23:28:13.363314] Train epoch [29/30], batch: [7800/16579], loss: 0.41722, accuracy: 0.99257, lr: 0.00017584, eta: 0:12:43
[2022-09-27 23:28:21.994316] Train epoch [29/30], batch: [7900/16579], loss: 0.41785, accuracy: 0.99253, lr: 0.00017584, eta: 0:12:09
[2022-09-27 23:28:30.601316] Train epoch [29/30], batch: [8000/16579], loss: 0.41785, accuracy: 0.99253, lr: 0.00017584, eta: 0:12:17
[2022-09-27 23:28:39.187316] Train epoch [29/30], batch: [8100/16579], loss: 0.41805, accuracy: 0.99254, lr: 0.00017584, eta: 0:12:09
[2022-09-27 23:28:47.756316] Train epoch [29/30], batch: [8200/16579], loss: 0.41818, accuracy: 0.99253, lr: 0.00017584, eta: 0:12:00
[2022-09-27 23:28:56.307315] Train epoch [29/30], batch: [8300/16579], loss: 0.41840, accuracy: 0.99254, lr: 0.00017584, eta: 0:12:00
[2022-09-27 23:29:04.894316] Train epoch [29/30], batch: [8400/16579], loss: 0.41846, accuracy: 0.99252, lr: 0.00017584, eta: 0:11:43
[2022-09-27 23:29:13.453317] Train epoch [29/30], batch: [8500/16579], loss: 0.41875, accuracy: 0.99251, lr: 0.00017584, eta: 0:11:26
[2022-09-27 23:29:22.067316] Train epoch [29/30], batch: [8600/16579], loss: 0.41874, accuracy: 0.99251, lr: 0.00017584, eta: 0:11:18
[2022-09-27 23:29:30.684315] Train epoch [29/30], batch: [8700/16579], loss: 0.41891, accuracy: 0.99251, lr: 0.00017584, eta: 0:11:09
[2022-09-27 23:29:39.315317] Train epoch [29/30], batch: [8800/16579], loss: 0.41922, accuracy: 0.99248, lr: 0.00017584, eta: 0:11:16
[2022-09-27 23:29:47.876316] Train epoch [29/30], batch: [8900/16579], loss: 0.41943, accuracy: 0.99246, lr: 0.00017584, eta: 0:11:08
[2022-09-27 23:29:56.480315] Train epoch [29/30], batch: [9000/16579], loss: 0.41980, accuracy: 0.99245, lr: 0.00017584, eta: 0:10:51
[2022-09-27 23:30:05.065315] Train epoch [29/30], batch: [9100/16579], loss: 0.42007, accuracy: 0.99244, lr: 0.00017584, eta: 0:10:43
[2022-09-27 23:30:13.656316] Train epoch [29/30], batch: [9200/16579], loss: 0.42026, accuracy: 0.99244, lr: 0.00017584, eta: 0:10:49
[2022-09-27 23:30:22.242315] Train epoch [29/30], batch: [9300/16579], loss: 0.42044, accuracy: 0.99243, lr: 0.00017584, eta: 0:10:26
[2022-09-27 23:30:30.827314] Train epoch [29/30], batch: [9400/16579], loss: 0.42053, accuracy: 0.99242, lr: 0.00017584, eta: 0:10:03
[2022-09-27 23:30:39.429316] Train epoch [29/30], batch: [9500/16579], loss: 0.42096, accuracy: 0.99241, lr: 0.00017584, eta: 0:10:08
[2022-09-27 23:30:48.065318] Train epoch [29/30], batch: [9600/16579], loss: 0.42099, accuracy: 0.99241, lr: 0.00017584, eta: 0:09:53
[2022-09-27 23:30:56.659317] Train epoch [29/30], batch: [9700/16579], loss: 0.42097, accuracy: 0.99243, lr: 0.00017584, eta: 0:09:51
[2022-09-27 23:31:05.256315] Train epoch [29/30], batch: [9800/16579], loss: 0.42108, accuracy: 0.99242, lr: 0.00017584, eta: 0:09:42
[2022-09-27 23:31:13.833315] Train epoch [29/30], batch: [9900/16579], loss: 0.42139, accuracy: 0.99240, lr: 0.00017584, eta: 0:09:14
[2022-09-27 23:31:22.397316] Train epoch [29/30], batch: [10000/16579], loss: 0.42148, accuracy: 0.99240, lr: 0.00017584, eta: 0:09:25
[2022-09-27 23:31:30.970313] Train epoch [29/30], batch: [10100/16579], loss: 0.42165, accuracy: 0.99239, lr: 0.00017584, eta: 0:09:10
[2022-09-27 23:31:39.565313] Train epoch [29/30], batch: [10200/16579], loss: 0.42172, accuracy: 0.99239, lr: 0.00017584, eta: 0:09:14
[2022-09-27 23:31:48.212317] Train epoch [29/30], batch: [10300/16579], loss: 0.42192, accuracy: 0.99240, lr: 0.00017584, eta: 0:08:47
[2022-09-27 23:31:56.873315] Train epoch [29/30], batch: [10400/16579], loss: 0.42201, accuracy: 0.99240, lr: 0.00017584, eta: 0:08:57
[2022-09-27 23:32:05.556317] Train epoch [29/30], batch: [10500/16579], loss: 0.42184, accuracy: 0.99241, lr: 0.00017584, eta: 0:08:48
[2022-09-27 23:32:14.252316] Train epoch [29/30], batch: [10600/16579], loss: 0.42209, accuracy: 0.99240, lr: 0.00017584, eta: 0:08:40
[2022-09-27 23:32:22.919316] Train epoch [29/30], batch: [10700/16579], loss: 0.42262, accuracy: 0.99238, lr: 0.00017584, eta: 0:08:31
[2022-09-27 23:32:31.578317] Train epoch [29/30], batch: [10800/16579], loss: 0.42274, accuracy: 0.99239, lr: 0.00017584, eta: 0:08:22
[2022-09-27 23:32:40.249318] Train epoch [29/30], batch: [10900/16579], loss: 0.42282, accuracy: 0.99239, lr: 0.00017584, eta: 0:08:14
[2022-09-27 23:32:48.921316] Train epoch [29/30], batch: [11000/16579], loss: 0.42292, accuracy: 0.99239, lr: 0.00017584, eta: 0:07:31
[2022-09-27 23:32:57.611317] Train epoch [29/30], batch: [11100/16579], loss: 0.42314, accuracy: 0.99238, lr: 0.00017584, eta: 0:07:45
[2022-09-27 23:33:06.295315] Train epoch [29/30], batch: [11200/16579], loss: 0.42326, accuracy: 0.99238, lr: 0.00017584, eta: 0:07:42
[2022-09-27 23:33:14.931316] Train epoch [29/30], batch: [11300/16579], loss: 0.42360, accuracy: 0.99235, lr: 0.00017584, eta: 0:07:44
[2022-09-27 23:33:23.523319] Train epoch [29/30], batch: [11400/16579], loss: 0.42366, accuracy: 0.99236, lr: 0.00017584, eta: 0:07:30
[2022-09-27 23:33:32.106314] Train epoch [29/30], batch: [11500/16579], loss: 0.42369, accuracy: 0.99235, lr: 0.00017584, eta: 0:07:16
[2022-09-27 23:33:40.646317] Train epoch [29/30], batch: [11600/16579], loss: 0.42365, accuracy: 0.99237, lr: 0.00017584, eta: 0:07:03
[2022-09-27 23:33:49.240317] Train epoch [29/30], batch: [11700/16579], loss: 0.42394, accuracy: 0.99234, lr: 0.00017584, eta: 0:06:59
[2022-09-27 23:33:57.811313] Train epoch [29/30], batch: [11800/16579], loss: 0.42406, accuracy: 0.99234, lr: 0.00017584, eta: 0:06:46
[2022-09-27 23:34:06.410314] Train epoch [29/30], batch: [11900/16579], loss: 0.42411, accuracy: 0.99234, lr: 0.00017584, eta: 0:06:37
[2022-09-27 23:34:14.961315] Train epoch [29/30], batch: [12000/16579], loss: 0.42406, accuracy: 0.99236, lr: 0.00017584, eta: 0:06:29
[2022-09-27 23:34:23.567317] Train epoch [29/30], batch: [12100/16579], loss: 0.42418, accuracy: 0.99236, lr: 0.00017584, eta: 0:06:20
[2022-09-27 23:34:32.203317] Train epoch [29/30], batch: [12200/16579], loss: 0.42428, accuracy: 0.99236, lr: 0.00017584, eta: 0:06:25
[2022-09-27 23:34:40.872316] Train epoch [29/30], batch: [12300/16579], loss: 0.42432, accuracy: 0.99236, lr: 0.00017584, eta: 0:05:38
[2022-09-27 23:34:49.588315] Train epoch [29/30], batch: [12400/16579], loss: 0.42438, accuracy: 0.99236, lr: 0.00017584, eta: 0:06:03
[2022-09-27 23:34:58.265316] Train epoch [29/30], batch: [12500/16579], loss: 0.42449, accuracy: 0.99236, lr: 0.00017584, eta: 0:05:50
[2022-09-27 23:35:06.920316] Train epoch [29/30], batch: [12600/16579], loss: 0.42462, accuracy: 0.99237, lr: 0.00017584, eta: 0:05:42
[2022-09-27 23:35:15.558317] Train epoch [29/30], batch: [12700/16579], loss: 0.42485, accuracy: 0.99235, lr: 0.00017584, eta: 0:05:45
[2022-09-27 23:35:24.229313] Train epoch [29/30], batch: [12800/16579], loss: 0.42486, accuracy: 0.99236, lr: 0.00017584, eta: 0:05:24
[2022-09-27 23:35:32.878314] Train epoch [29/30], batch: [12900/16579], loss: 0.42503, accuracy: 0.99235, lr: 0.00017584, eta: 0:05:20
[2022-09-27 23:35:41.510314] Train epoch [29/30], batch: [13000/16579], loss: 0.42515, accuracy: 0.99235, lr: 0.00017584, eta: 0:04:42
[2022-09-27 23:35:50.165313] Train epoch [29/30], batch: [13100/16579], loss: 0.42517, accuracy: 0.99234, lr: 0.00017584, eta: 0:05:02
[2022-09-27 23:35:59.035316] Train epoch [29/30], batch: [13200/16579], loss: 0.42530, accuracy: 0.99233, lr: 0.00017584, eta: 0:05:17
[2022-09-27 23:36:07.709314] Train epoch [29/30], batch: [13300/16579], loss: 0.42526, accuracy: 0.99233, lr: 0.00017584, eta: 0:04:41
[2022-09-27 23:36:16.329316] Train epoch [29/30], batch: [13400/16579], loss: 0.42547, accuracy: 0.99233, lr: 0.00017584, eta: 0:04:33
[2022-09-27 23:36:24.943314] Train epoch [29/30], batch: [13500/16579], loss: 0.42565, accuracy: 0.99231, lr: 0.00017584, eta: 0:04:24
[2022-09-27 23:36:33.577318] Train epoch [29/30], batch: [13600/16579], loss: 0.42574, accuracy: 0.99230, lr: 0.00017584, eta: 0:04:16
[2022-09-27 23:36:42.239316] Train epoch [29/30], batch: [13700/16579], loss: 0.42577, accuracy: 0.99230, lr: 0.00017584, eta: 0:04:10
[2022-09-27 23:36:50.856316] Train epoch [29/30], batch: [13800/16579], loss: 0.42584, accuracy: 0.99231, lr: 0.00017584, eta: 0:03:56
[2022-09-27 23:36:59.460315] Train epoch [29/30], batch: [13900/16579], loss: 0.42591, accuracy: 0.99230, lr: 0.00017584, eta: 0:03:55
[2022-09-27 23:37:08.270314] Train epoch [29/30], batch: [14000/16579], loss: 0.42605, accuracy: 0.99230, lr: 0.00017584, eta: 0:03:44
[2022-09-27 23:37:16.836316] Train epoch [29/30], batch: [14100/16579], loss: 0.42595, accuracy: 0.99230, lr: 0.00017584, eta: 0:03:35
[2022-09-27 23:37:25.422315] Train epoch [29/30], batch: [14200/16579], loss: 0.42617, accuracy: 0.99229, lr: 0.00017584, eta: 0:03:22
[2022-09-27 23:37:34.044313] Train epoch [29/30], batch: [14300/16579], loss: 0.42627, accuracy: 0.99229, lr: 0.00017584, eta: 0:03:11
[2022-09-27 23:37:42.882316] Train epoch [29/30], batch: [14400/16579], loss: 0.42642, accuracy: 0.99229, lr: 0.00017584, eta: 0:03:13
[2022-09-27 23:37:51.576314] Train epoch [29/30], batch: [14500/16579], loss: 0.42644, accuracy: 0.99229, lr: 0.00017584, eta: 0:03:02
[2022-09-27 23:38:00.244316] Train epoch [29/30], batch: [14600/16579], loss: 0.42659, accuracy: 0.99229, lr: 0.00017584, eta: 0:02:54
[2022-09-27 23:38:08.908319] Train epoch [29/30], batch: [14700/16579], loss: 0.42662, accuracy: 0.99228, lr: 0.00017584, eta: 0:02:39
[2022-09-27 23:38:17.584317] Train epoch [29/30], batch: [14800/16579], loss: 0.42682, accuracy: 0.99227, lr: 0.00017584, eta: 0:02:32
[2022-09-27 23:38:26.270315] Train epoch [29/30], batch: [14900/16579], loss: 0.42682, accuracy: 0.99227, lr: 0.00017584, eta: 0:02:24
[2022-09-27 23:38:35.000314] Train epoch [29/30], batch: [15000/16579], loss: 0.42701, accuracy: 0.99224, lr: 0.00017584, eta: 0:02:17
[2022-09-27 23:38:43.714316] Train epoch [29/30], batch: [15100/16579], loss: 0.42716, accuracy: 0.99224, lr: 0.00017584, eta: 0:02:14
[2022-09-27 23:38:52.417315] Train epoch [29/30], batch: [15200/16579], loss: 0.42731, accuracy: 0.99224, lr: 0.00017584, eta: 0:01:57
[2022-09-27 23:39:01.070315] Train epoch [29/30], batch: [15300/16579], loss: 0.42728, accuracy: 0.99224, lr: 0.00017584, eta: 0:01:49
[2022-09-27 23:39:09.799315] Train epoch [29/30], batch: [15400/16579], loss: 0.42742, accuracy: 0.99222, lr: 0.00017584, eta: 0:01:42
[2022-09-27 23:39:18.463315] Train epoch [29/30], batch: [15500/16579], loss: 0.42758, accuracy: 0.99221, lr: 0.00017584, eta: 0:01:33
[2022-09-27 23:39:27.131316] Train epoch [29/30], batch: [15600/16579], loss: 0.42764, accuracy: 0.99221, lr: 0.00017584, eta: 0:01:23
[2022-09-27 23:39:35.753314] Train epoch [29/30], batch: [15700/16579], loss: 0.42779, accuracy: 0.99220, lr: 0.00017584, eta: 0:01:14
[2022-09-27 23:39:44.572316] Train epoch [29/30], batch: [15800/16579], loss: 0.42787, accuracy: 0.99220, lr: 0.00017584, eta: 0:01:01
[2022-09-27 23:39:53.183315] Train epoch [29/30], batch: [15900/16579], loss: 0.42798, accuracy: 0.99220, lr: 0.00017584, eta: 0:00:57
[2022-09-27 23:40:01.757314] Train epoch [29/30], batch: [16000/16579], loss: 0.42803, accuracy: 0.99220, lr: 0.00017584, eta: 0:00:49
[2022-09-27 23:40:10.543316] Train epoch [29/30], batch: [16100/16579], loss: 0.42807, accuracy: 0.99220, lr: 0.00017584, eta: 0:00:41
[2022-09-27 23:40:19.107316] Train epoch [29/30], batch: [16200/16579], loss: 0.42808, accuracy: 0.99220, lr: 0.00017584, eta: 0:00:32
[2022-09-27 23:40:27.643318] Train epoch [29/30], batch: [16300/16579], loss: 0.42817, accuracy: 0.99219, lr: 0.00017584, eta: 0:00:24
[2022-09-27 23:40:36.192314] Train epoch [29/30], batch: [16400/16579], loss: 0.42820, accuracy: 0.99219, lr: 0.00017584, eta: 0:00:15
[2022-09-27 23:40:44.736318] Train epoch [29/30], batch: [16500/16579], loss: 0.42825, accuracy: 0.99218, lr: 0.00017584, eta: 0:00:06
======================================================================
[2022-09-27 23:42:24.272373] Test 29, accuracy: 0.97820 time: 0:01:30
======================================================================

进程已结束,退出代码0
